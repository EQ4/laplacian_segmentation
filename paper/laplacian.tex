% -----------------------------------------------
% Template for ISMIR 2014
% (based on earlier ISMIR templates)
% -----------------------------------------------

\documentclass{article}
\usepackage{ismir,cite}
\usepackage{graphicx}
\usepackage{booktabs}

\usepackage{brian}

% Title.
% ------
\title{Analyzing song structure with spectral clustering}

% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
\twoauthors%
 {Brian McFee} {Center for Jazz Studies \\ Columbia University \\ {\tt brm2132@columbia.edu} }
 {Daniel P.W. Ellis} {LabROSA \\ Columbia University \\ {\tt dpwe@ee.columbia.edu }}

% Three addresses
% --------------
% \threeauthors
%   {Brian McFee} {Affiliation1 \\ {\tt author1@ismir.edu}}
%   {Second author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
%   {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}

% Four addresses
% --------------
%\fourauthors
%  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
%  {Second author}{Affiliation2 \\ {\tt author2@ismir.edu}}
%  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}
%  {Fourth author} {Affiliation4 \\ {\tt author4@ismir.edu}}

\begin{document}
%
\maketitle
%
\begin{abstract}
Many approaches to analyzing the structure of a musical recording involve detecting
sequential patterns within a self-similarity matrix derived from time-series features.
Such patterns ideally capture repeated sequences, which then form the building blocks
of large-scale structure.

In this work, techniques from spectral graph theory are applied to analyze repeated
patterns in musical recordings.  The proposed method produces a low-dimensional
encoding of repetition structure, and exposes the hierarchical relationships among
structural components at differing levels of granularity.  Finally, we demonstrate how
to apply the proposed method to the task of music segmentation.
\end{abstract}
%
\section{Introduction}
\label{sec:introduction}
Detecting repeated forms in audio is fundamental to the analysis of structure in many 
forms of music.  While small-scale repetitions --- such as instances of an
individual chord --- are simple to detect, accurately combining multiple 
small-scale repetitions into larger structures is a challenging algorithmic task.
Much of the current research on this topic begins by calculating local, frame-wise
similarities over acoustic features (usually harmonic), and then searching for
patterns in the all-pairs self-similarity matrix~\cite{foote2000automatic}.

In the majority of existing work on structural segmentation, the analysis is
\emph{flat}, in the sense that the representation does not explicitly encode nesting
or hierarchical structure in the repeated forms.  Instead, novelty curves are commonly
used to detect transitions between sections. 

\subsection{Our contributions}
In this paper, we formulate the structure analysis problem in the context of spectral
graph theory.  By combining local consistency cues with long-term repetition encodings
and analyzing the eigenvectors of the resulting graph Laplacian, we produce a compact
representation that effectively encodes repetition structure at multiple levels of
granularity.  To effectively link repeating sequences, we formulate an optimally
weighted combination of local timbre consistency and long-term repetition descriptors.

To motivate the analysis technique, we demonstrate its use for the
standard task of flat structural annotation.  However, we emphasize that the approach
itself can be applied more generally to analyze structure at multiple resolutions.

\subsection{Related work}

The structural repetition features used in this work are inspired by those of
Serr\`{a}~\etal~\cite{serra2014unsupervised}, wherein structure is detected by 
applying filtering operators to a lag-skewed self-similarity matrix.  The primary
deviation in this work is the graphical interpretation and subsequent analysis of 
the filtered self-similarity matrix.

% kaiser proposed a method to combine input features for novelty detection
% our method is different: we optimally combine local timbre information with global
% harmonic repetition
Recently, Kaiser~\etal{} demonstrated a method to combine tonal and timbral features for
structural boundary detection~\cite{kaiser2013simple}.  Whereas their method forms a
novelty curve from the combination of multiple features, our feature combination
differs by using local timbre consistency to build internal connections among 
sequences of long-range tonal repetitions.

% grohganz used spectral analysis of self-similarity to detect diagonals
% but our approach is more direct, analyzes the diffusion process rather than the
% graph itself
Our general approach is similar in spirit to that of Grohganz 
\etal~\cite{grohganz2013converting}, in which diagonal bands 
of a self-similarity matrix are expanded into block structures by 
spectral analysis.  Their method analyzed the spectral decomposition of the 
self-similarity matrix directly, whereas the method proposed here operates on the
graph Laplacian.  Similarly, Kaiser and Sikora applied non-negative matrix
factorization directly to a self-similarity matrix in order to detect blocks of
repeating elements~\cite{kaiser2010music}.  
As we will demonstrate, the Laplacian provides a more direct means to
expose block structure at multiple levels of detail.

\section{Graphical repetition encoding}
% Begin with nearest-neighbor linkage in some feature space
% Apply diagonal majority vote filtering
% Add the conveyor belt links
% Compute the graph Laplacian, observe magic

Our general structural analysis strategy is to construct and partition a graph
over time points (samples) in the song.
Let $X = [x_1, x_2, \dots, x_n] \in \R^{d\times n}$ denote a $d$-dimensional time
series feature matrix, \eg, a chromagram or sequence of Mel-frequency cepstral 
coefficients.  As a first step toward detecting and representing repetition structure, 
we form a \emph{binary recurrence matrix} $R \in {\{0,1\}}^{n\times n}$, where 
\begin{equation}
R_{ij}(X) \defeq \begin{cases}
1 & x_i, x_j \text{ are mutual $k$-nearest neighbors}\\
0 & \text{otherwise},
\end{cases}
\label{unfiltered-rep}
\end{equation}
and $k > 0$ parameterizes the degree of connectivity.

Ideally, repeated structures should appear as diagonal stripes in $R$.
In practice, it is beneficial to apply a smoothing filter to suppress erroneous
links and fill in gaps.  We apply a windowed majority vote to each diagonal of
$R$, resulting in the filtered matrix $R'$:
\begin{equation}
R'_{ij} \defeq \maj\left\{ R_{i+t, j+t} \given t \in -w, -w+1, \dots, w\right\},
\label{filtered-rep}
\end{equation}
where $w$ is a discrete parameter that defines the minimum length of a valid
repetition sequence.

\subsection{Internal connectivity}
The filtered recurrence matrix $R'$ can be interpreted as an unweighted, undirected 
graph, whose vertices correspond to samples (columns of $X$), and edges correspond 
to equivalent position within a repeated sequence. Note, however, that successive 
positions $(i, i+1)$ will not generally be connected in $R'$, so the constituent 
samples of a particular sequence may not be connected.

To facilitate discovery of repeated sections, edges between adjacent samples $(i, i+1)$
and $(i, i-1)$ are introduced, resulting in the
\emph{sequence-augmented graph} $R^+$:
\begin{align}
\Delta_{ij} &\defeq \begin{cases}
1 & |i - j| = 1\\
0 & \text{otherwise}
\end{cases},\\
R^+_{ij} &\defeq \max(\Delta_{ij}, R'_{ij})\label{pathmatrix}.
\end{align}
With appropriate normalization, $R^+$ characterizes a Markov process over samples, 
where at each step $i$, the process either moves to an adjacent sample $i\pm1$, or 
a random repetition of $i$; a process exemplified by the 
Infinite Jukebox~\cite{infinitejukebox}.

\Cref{pathmatrix} combines local temporal connectivity with long-term
recurrence information.  Ideally, edges would exist only between pairs $\{i,j\}$
belonging to the same structural component, but of course, this information is hidden.  
The added edges along the first diagonals create a fully connected graph, but due to
recurrence links, repeated sections will exhibit additional internal connectivity.  
Let $i$ and $j$ denote two repetitions of the same sample at different times; then $R^+$ 
should contain sequential edges $\{i, i+1\}$, $\{j, j+1\}$ and repetition edges $\{i,
j\}$, $\{i+1, j+1\}$. On the other hand, unrelated sections with no repetition edges
can only connect via sequence edges.
% Connections between unrelated blocks are likely to remain sparse, owing primarily to
% cross-section links along the first diagonals.  

% $R^+$ encodes the coarse similarity structure
% For the purposes of musical structure analysis, we would like to produce a block
% matrix $C$, where $C_{i,j}$ indicates that $i$ and $j$ belong to the same structural
% component (\eg, $i$ and $j$ are both of type \emph{verse}).

\subsection{Balancing local and global linkage}
The construction of \cref{pathmatrix} describes the intuition behind combining
local sequential connections with global repetition structure, but it does not balance the 
two competing goals.  Long tracks with many repetitions can produce recurrence links which 
vastly outnumber local connectivity connections.
In this regime, partitioning into contiguous sections becomes difficult, and subsequent 
analysis of the graph may fail to detect sequential structure.

If we allow (non-negative) weights on the edges, then the combination can be parameterized 
by a weighting parameter $\mu \in [0, 1]$:
\begin{equation}
R_{ij}^\mu \defeq \mu R_{ij}' + (1 - \mu) \Delta_{ij}. \label{weightedsum}
\end{equation}

This raises the question: how should $\mu$ be set?  Returning to the motivating
example of the random walk, we opt for a process that on average, tends to
move either in sequence or across (all) repetitions with equal probability.  In terms of
$\mu$, this indicates that the combination should assign equal weight to the local and
repetition edges.
This suggests a balancing objective for all frames $i$:
$$
\mu \sum_j R'_{ij} \approx (1-\mu) \sum_j \Delta_{ij}.
$$
Minimizing the average squared error between the two terms above leads to the
following quadratic optimization:
\begin{equation}
\min_{\mu \in [0, 1]} \frac{1}{2} \sum_i {(\mu d_i(R') - (1 - \mu)d_i(\Delta))}^2,\label{optweight}
\end{equation}
where $d_i(G) \defeq \sum_j G_{ij}$ denotes the degree (sum of incident edge-weights) 
of $i$ in $G$. Treating $d(\cdot) \defeq {[d_i(\cdot)]}_{i=1}^n$ as a vector in 
$\R^n_+$ yields the optimal solution to \cref{optweight}:
\begin{equation}
\mu^* = \frac{\langle d(\Delta), d(R') + d(\Delta)\rangle}{\|d(R') +
d(\Delta)\|^2}.\label{optweight:solution}
\end{equation}
Note that because $\Delta$ is non-empty (contains at least one edge), it follows that $\|d(\Delta)\|^2 > 0$,
which implies $\mu^* > 0$. Similarly, if $R'$ is non-empty, then $\mu^* < 1$, and the resulting combination 
retains the full connectivity structure of the unweighted $R^+$ (\cref{pathmatrix}).

\subsection{Edge weighting and feature fusion}
The construction above relies upon a single feature representation to 
determine the self-similarity structure, and uses constant edge weights for the
repetition and local edges.  This can be generalized to support
feature-weighted edges by replacing $R'$ with a masked similarity matrix:
\begin{equation}
R'_{ij} \mapsto R'_{ij} S_{ij},
\end{equation}
where $S_{ij}$ denotes a non-negative affinity between frames $i$ and $j$, \eg,
a Gaussian kernel over feature vectors $x_i, x_j$:
\[
S^\text{rep}_{ij} \defeq \exp\left(-\frac{1}{2\sigma^2}\|x_i - x_j\|^2\right)
\]

Similarly, $\Delta$ can be replaced with a weighted sequence graph.  However, in doing so,
care must be taken when selecting the affinity function.  The same features used to detect 
repetition (typically harmonic in nature) may not capture local consistency, since 
successive frames do not generally retain harmonic similarity.  

Recent work has demonstrated that local timbre differences 
can provide an effective cue for structural boundary
detection~\cite{kaiser2013simple}.  This motivates the use of two contrasting feature
descriptors: harmonic features for detecting long-range repeating forms, and timbral
features for detecting local consistency.  We assume that these features are
respectively supplied in the form of affinity matrices $S^\text{rep}$ and
$S^\text{loc}$.  Combining these affinities with the detected repetition structure and
optimal weighting yields the \emph{sequence-augmented affinity matrix} $A$:
\begin{equation}
A_{ij} \defeq \mu R'_{ij} S^\text{rep}_{ij} + (1-\mu) \Delta_{ij} S^\text{loc}_{ij},\label{saam}
\end{equation}
where $R'$ is understood to be
constructed solely from the repetition affinities $S^\text{rep}$, and $\mu$ is
optimized by solving~\eqref{optweight:solution} with the weighted affinity matrices.


\section{Graph partitioning and structural~analysis}

The Laplacian is a fundamental tool in the field of spectral graph
theory, as it can be interpreted as a discrete analog of a diffusion operator over the
vertices of the graph, and its spectrum can be used to characterize 
vertex connectivity~\cite{chung1997spectral}.  This section describes in detail
how spectral clustering can be used to analyze and partition the repetition graph 
constructed in the previous section, and reveal musical structure.

\subsection{Background: spectral clustering}
Let $D$ denote the diagonal \emph{degree matrix} of $A$: 
$${D \defeq \diag(d(A))}.$$
The \emph{symmetric normalized Laplacian} $L$ is defined as:
\begin{equation}
L \defeq I - D^{-1/2} A D^{-1/2}.\label{symlaplacian}
\end{equation}

The Laplacian forms the basis of spectral clustering, in which vertices are
represented in terms of the eigenvectors of $L$~\cite{von2007tutorial}.  More
specifically, to partition a graph into $m$ components, each point $i$ is represented as
the vector of the $i$th coordinates of the first $m$ eigenvectors of $L$,
corresponding to the $m$ smallest eigenvalues.\footnote{An additional
length-normalization is applied to each vector, to correct for scaling introduced by
the symmetric normalized Laplacian~\cite{von2007tutorial}.}
The motivation for this method stems from the observation that the multiplicity of the 
bottom eigenvalue $\lambda_0 = 0$ corresponds to the number of connected components in a
graph, and the corresponding eigenvectors encode component memberships amongst vertices.

In the non-ideal case, the graph is fully connected, so $\lambda_0$ has multiplicity 1, 
and the bottom eigenvector trivially encodes membership in the graph. 
However, in the case of $A$, we expect there to be many components
with high intra-connectivity and relatively small inter-connectivity at the
transition points between sections.  Spectral clustering can be viewed as an approximation 
method for finding normalized graph cuts~\cite{von2007tutorial}, and it is well-suited to detecting and pruning these weak links.

\Cref{recurrence} illustrates an example of the encoding produced by spectral
decomposition of $L$.  Although the first eigenvector (column) is uninformative, 
the remaining bases clearly encode membership in the diagonal regions depicted in the 
affinity matrix.  
The resulting pair-wise frame similarities for this example are shown in \Cref{lowrank}, 
which clearly demonstrates the ability of this representation to iteratively reveal 
nested repeating structure.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figs/recurrence}
\caption{Left: the recurrence matrix $R$ for \emph{The Beatles --- Come
Together}. Center: the sequence-augmented affinity matrix $A$.
Right: the first 10 basis features (columns), ordered left-to-right.  
The leading columns encode the primary structural components, while subsequent
components encode refinements.\label{recurrence}}
\end{figure*}

 
\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figs/lowrank}
\caption{Pair-wise frame similarities $\left(YY\trans\right)$ using the first $10$ components for \emph{The Beatles --- Come Together}.  The first
(trivial) component ($m=1$) captures the entire song, and the second ($m=2$) separates
the outro (final vamp) from the rest of the song.  
Subsequent refinements separate the solo, refrain, verse, and outro, and then individual measures.\label{lowrank}}
\end{figure*}

To apply spectral clustering, we will use $k$-means clustering with the (normalized) 
eigenvectors $Y \in \R^{n \times M}$ as features, where $M > 0$ is a 
specified maximum number of structural component types.  Varying $M$ --- equivalently, 
the dimension of the representation --- directly controls the granularity of 
the resulting segmentation.


\subsection{Boundary detection}
For a fixed number of segment types $m$, segment boundaries can estimated 
by clustering the rows of $Y$ after truncating to the first $m$ dimensions.
After clustering, segment boundaries are detected by searching for change-points in
the cluster assignments.  This method is formalized in \Cref{bd}.
Note that the number of segment \emph{types} is distinct from the number of \emph{segments} because a single 
type (\eg, verse) may repeat multiple times throughout the track.

\begin{algorithm}[t]
\caption{Boundary detection}\label{bd}
\begin{algorithmic}[1]
\Require{Laplacian eigenvectors $Y \in \R^{n \times m}$, }
\Ensure{Boundaries $b$, segment labels $c \in {[m]}^n$}
\Function{Boundary-detect}{$Y$}
    \State{$\hat{y}_i \leftarrow Y_{i, \cdot} / \|Y_{i, \cdot}\|$}
    \Comment{Normalize each row $Y_{i, \cdot}$}
    \State{Run $k$-means on ${\{\hat{y}_i\}}_{i=1}^n$ with $k=m$}
    \State{Let $c_i$ denote the cluster containing $\hat{y}_i$}
    \State{$b \leftarrow \{i \given c_i \neq c_{i+1}\}$} 
    \State{\Return{$(b, c)$}}
\EndFunction{}
\end{algorithmic}
\end{algorithm}

\subsection{Laplacian structural decomposition}

To decompose an input song into its structural components, we propose a method, listed
as \Cref{lsd},
to find boundaries and structural annotations at multiple levels of structural
complexity.  \Cref{lsd} first computes the Laplacian as described above, and
then iteratively increases the set of eigenvectors for use in \Cref{bd}.  For $m=2$,
the first two eigenvectors --- corresponding to the two smallest eigenvalues of $L$
--- are taken.  In general, for $m$ types of repeating component, the bottom $m$
eigenvectors are used to label frames and detect boundaries.  The result is a sequence
of boundaries $B^m$ and frame labels $C^m$, for values $m \in 2, 3, \dots, M$.

Note that unlike most structural analysis algorithms, \Cref{lsd} does not produce a
single decomposition of the song, but rather a sequence of decompositions ordered by 
increasing complexity.  This property can be beneficial in visualization applications,
where a user may be interested in the relationship between structural components at
multiple levels.  Similarly, in interactive display applications, a user may request 
more or less detailed analyses for a track.  Since complexity is controlled by
a single, discrete parameter $M$, this application is readily supported with a minimal
set of interface controls (\eg, a slider).

However, for standardized evaluation, the method must produce a single, flat
segmentation.  Adaptively estimating the appropriate level of analysis in
this context is somewhat ill-posed, as different use-cases require differing levels of 
detail.  We apply a simple selection criterion based on the level of detail
commonly observed in standard datasets~\cite{harte2010towards,smith2011design}.  
First, the set of candidates is reduced to those in which the mean segment duration is 
at least 10 seconds.  
Subject to this constraint, the segmentation level $\tilde{m}$ is selected to maximize
frame-level annotation entropy.  This strategy tends to produce solutions with 
approximately balanced distributions over the set of segment types.  




\begin{algorithm}[t]
\caption{Laplacian structural decomposition\label{lsd}}
\begin{algorithmic}[1]
\Require{ Affinities: $S^\text{rep}, S^\text{loc} \in \R_+^{n\times n}$, 
maximum number of segment types $0 < M \leq n$}
\Ensure{Boundaries $B^m$ and frame labels $C^m$ for ${m \in 2\dots M}$}
\Function{\sc LSD}{$S^\text{rep}, S^\text{loc}, M$}
\State{$R \leftarrow $ \cref{unfiltered-rep} on $S^\text{rep}$}
\Comment{Recurrence detection}
\State{$R' \leftarrow $ \cref{filtered-rep} on $R$} \Comment{Recurrence filtering}
\State{$A \leftarrow $ \cref{saam}} \Comment{Sequence augmentation}
\State{$L \leftarrow I - D^{-1/2} A D^{-1/2}$}
\For{$m \in 2, 3, \dots, M$}
    \State{$Y \leftarrow$ bottom $m$ eigenvectors of $L$}
    \State{$(B^m, C^m) \leftarrow \text{\sc Boundary-Detect}(Y)$}
\EndFor{}
\State{\Return{${\{(B^m, C^m)\}}_{m=2}^M$}}
\EndFunction{}
\end{algorithmic}
\end{algorithm}

% For each m in 2 .. M:
% m <- argmax gap
% compute segment centroids over boundaries[m], and cluster by k-means with k=m
% return boundary times and segment labels

\section{Experiments}
To evaluate the proposed method quantitatively, we compare boundary detection and
structural annotation performance on two standard datasets.  We evaluate the
performance of the method using the automatic complexity estimation described above,
as well as performance achieved for each fixed value of $m$ across the dataset.

Finally, to evaluate the impact of the complexity estimation method, we compare to an
oracle model. For each track, a different $m^*$ is selected to maximize the
evaluation metric of interest.  This can be viewed as a simulation of interactive
visualization, in which the user has the freedom to dynamically adapt the level of
detail until she is satisfied.  Results in this setting may be interpreted as
measuring the best possible decomposition within the set produced by
\Cref{lsd}.

\subsection{Data and evaluation}
Our evaluation data is comprised of two sources:
\begin{description}
\item[Beatles-TUT:]
174 structurally annotated tracks from the Beatles corpus~\cite{paulus2006music}.  A single
annotation is provided for each track, and annotations generally correspond to
functional components (\eg, \emph{verse}, \emph{refrain}, or \emph{solo}).
\item[SALAMI:] 735 tracks from the SALAMI corpus~\cite{smith2011design}.  This corpus
spans a wide range of genres and instrumentation, and provides multiple
annotation levels for each track.  We report results on \emph{functional} 
and \emph{small-scale} annotations.
\end{description}

In each evaluation, we report the $F$-measure of boundary detection at 0.5-second and
3-second windows.  To evaluate structural annotation accuracy, we report pairwise frame 
classification $F$-measure~\cite{levy2008structural}.  
For comparison purposes, we report scores achieved by the method of 
Serr\`{a} \etal, denoted here as SMGA~\cite{serra2014unsupervised}.

\subsection{Implementation details}
All input signals are sampled at 22050Hz (mono), and analyzed with a 2048-sample FFT window
and 512-sample hop.  Repetition similarity matrices $S^\text{rep}$ were 
computed by first extracting log-power constant-Q spectrograms over 72 bins, ranging 
from $C2$ (32.7~Hz) to $C8$ (2093.0~Hz).

Constant-Q frames were mean-aggregated between detected beat events, and stacked using 
time-delay embedding with one step of history as in~\cite{serra2014unsupervised}.
Similarity matrices were then computed by applying a Gaussian kernel
to each pair of beat-synchronous frames $i$ and $j$.
The bandwidth parameter $\sigma^2$ was estimated by computing the average 
squared distance between each $x_i$ and its $k$th nearest neighbor, with 
$k$ set to $1 + \lceil 2\log_2 n\rceil$ (where $n$ denotes the number of detected beats).
The same $k$ was used to connect nearest neighbors when building the recurrence matrix $R$, 
with the additional constraint that frames cannot link to neighbors within 3 beats of each-other, which prevents
self-similar connections within the same measure. The majority vote window was set to
$w=17$.

Local timbre similarity $S^\text{loc}$ was computed by extracting the first 13 Mel
frequency cepstral coefficients (MFCC), mean-aggregating between detected beats, and
then applying a Gaussian kernel as done for $S^\text{rep}$.

All methods were implemented in Python with NumPy and 
librosa~\cite{van2011numpy, librosa}.

\subsection{Results}
The results of the evaluation are listed in
\Cref{results:beatles,results:salami:func,results:salami:small}.  For each fixed $m$, 
the scores are indicated as $L_m$.  $L$ indicates the automatic
maximum-entropy selector, and $L^*$ indicates the best possible $m$ for each metric
independently.

As a common trend across all data sets, the automatic $m$-selector often achieves results
comparable to the best fixed $m$.  However, it is consistently outperformed by the 
oracle model $L^*$, indicating that the output of \Cref{lsd} often contains 
accurate solutions, the automatic selector does not always choose them.

In the case of SALAMI (small), the automatic selector performs dramatically worse than
many of the fixed-$m$ methods, which may be explained by the relatively different
statistics of segment durations and numbers of unique segment types in the small-scale
annotations as compared to Beatles and SALAMI (functional).

To investigate whether a single $m$ could simultaneously optimize multiple evaluation
metrics for a given track, we plot the confusion matrices for the oracle selections on
SALAMI (functional) in \Cref{mconfusion}.  We observe that the
$m$ which optimizes $F_3$ is frequently larger than those for $F_{0.5}$ --- as indicated by
the mass in the lower triangle of the left plot --- or $F_\text{pair}$  --- as 
indicated by the upper triangle of the right plot.  
Although this observation depends upon our particular boundary-detection
strategy, it is corroborated by previous observations that the 0.5-second and 
3.0-second metrics evaluate qualitatively different objectives~\cite{smith2013meta}.
Consequently, it may be beneficial in practice to provide segmentations at multiple 
resolutions when the specific choice of evaluation criterion is unknown.

\begin{table}
\centering
\caption{Beatles (TUT)\label{results:beatles}}
\small
\begin{tabular}{lrrr}
\toprule
Method & $F_{0.5}$ & $F_3$ & $F_\text{pair}$\\
\midrule
$L_2$   & 0.307 $\pm$ 0.14 & 0.429 $\pm$ 0.18   & 0.576 $\pm$ 0.14\\
$L_3$   & 0.303 $\pm$ 0.15 & 0.544 $\pm$ 0.17   & 0.611 $\pm$ 0.13\\
$L_4$   & 0.307 $\pm$ 0.15 & 0.568 $\pm$ 0.17   & 0.616 $\pm$ 0.13\\
$L_5$   & 0.276 $\pm$ 0.14 & 0.553 $\pm$ 0.15   & 0.587 $\pm$ 0.12\\
$L_6$   & 0.259 $\pm$ 0.14 & 0.530 $\pm$ 0.15   & 0.556 $\pm$ 0.12\\
$L_7$   & 0.246 $\pm$ 0.13 & 0.507 $\pm$ 0.14   & 0.523 $\pm$ 0.12\\
$L_8$   & 0.229 $\pm$ 0.13 & 0.477 $\pm$ 0.15   & 0.495 $\pm$ 0.12\\
$L_9$   & 0.222 $\pm$ 0.12 & 0.446 $\pm$ 0.14   & 0.468 $\pm$ 0.12\\
$L_{10}$& 0.214 $\pm$ 0.11 & 0.425 $\pm$ 0.13   & 0.443 $\pm$ 0.12\\
\midrule
$L$     & 0.312 $\pm$ 0.15 & 0.579 $\pm$ 0.16   & 0.628 $\pm$ 0.13\\
$L^*$   & 0.414 $\pm$ 0.14 & 0.684 $\pm$ 0.13   & 0.694 $\pm$ 0.12\\
\midrule
SMGA    & 0.293 $\pm$ 0.13 & 0.699 $\pm$ 0.16   & 0.715 $\pm$ 0.15\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}
\centering
\caption{ SALAMI (Functions)\label{results:salami:func}}
\small
\begin{tabular}{lrrr}
\toprule
Method & $F_{0.5}$ & $F_3$ & $F_\text{pair}$\\
\midrule
$L_2$   & 0.324 $\pm$ 0.13 & 0.383 $\pm$ 0.15   & 0.539 $\pm$ 0.16\\
$L_3$   & 0.314 $\pm$ 0.13 & 0.417 $\pm$ 0.16   & 0.549 $\pm$ 0.13\\
$L_4$   & 0.303 $\pm$ 0.12 & 0.439 $\pm$ 0.16   & 0.547 $\pm$ 0.13\\
$L_5$   & 0.293 $\pm$ 0.12 & 0.444 $\pm$ 0.16   & 0.535 $\pm$ 0.12\\
$L_6$   & 0.286 $\pm$ 0.12 & 0.452 $\pm$ 0.16   & 0.521 $\pm$ 0.13\\
$L_7$   & 0.273 $\pm$ 0.11 & 0.442 $\pm$ 0.16   & 0.502 $\pm$ 0.13\\
$L_8$   & 0.267 $\pm$ 0.12 & 0.437 $\pm$ 0.16   & 0.483 $\pm$ 0.13\\
$L_9$   & 0.260 $\pm$ 0.11 & 0.443 $\pm$ 0.16   & 0.464 $\pm$ 0.14\\
$L_{10}$& 0.250 $\pm$ 0.11 & 0.422 $\pm$ 0.16   & 0.445 $\pm$ 0.14\\
\midrule
$L$     & 0.304 $\pm$ 0.13 & 0.455 $\pm$ 0.16   & 0.546 $\pm$ 0.14\\
$L^*$   & 0.406 $\pm$ 0.13 & 0.579 $\pm$ 0.15   & 0.652 $\pm$ 0.13\\
\midrule
SMGA    & 0.224 $\pm$ 0.11 & 0.550 $\pm$ 0.18   & 0.553 $\pm$ 0.15\\
\bottomrule
\end{tabular}
\end{table}

% \begin{table}
% \centering
% \caption{ SALAMI (Large)\label{results:salami:large}}
% \begin{tabular}{lrrr}
% \toprule
% Method & $F_{0.5}$ & $F_3$ & $F_\text{pair}$\\
% \midrule
% $L_2$   & 0.311 $\pm$ 0.12 & 0.371 $\pm$ 0.14   & 0.591 $\pm$ 0.15\\
% $L_3$   & 0.309 $\pm$ 0.12 & 0.415 $\pm$ 0.16   & 0.593 $\pm$ 0.14\\
% $L_4$   & 0.304 $\pm$ 0.12 & 0.444 $\pm$ 0.16   & 0.579 $\pm$ 0.13\\
% $L_5$   & 0.297 $\pm$ 0.12 & 0.455 $\pm$ 0.16   & 0.558 $\pm$ 0.14\\
% $L_6$   & 0.291 $\pm$ 0.12 & 0.465 $\pm$ 0.16   & 0.539 $\pm$ 0.14\\
% $L_7$   & 0.279 $\pm$ 0.11 & 0.457 $\pm$ 0.16   & 0.513 $\pm$ 0.14\\
% $L_8$   & 0.273 $\pm$ 0.11 & 0.451 $\pm$ 0.15   & 0.489 $\pm$ 0.14\\
% $L_9$   & 0.267 $\pm$ 0.11 & 0.448 $\pm$ 0.15   & 0.467 $\pm$ 0.14\\
% $L_{10}$& 0.257 $\pm$ 0.11 & 0.437 $\pm$ 0.15   & 0.444 $\pm$ 0.14\\
% \midrule
% $L$     & 0.303 $\pm$ 0.13 & 0.462 $\pm$ 0.16   & 0.569 $\pm$ 0.15\\
% $L^*$   & 0.401 $\pm$ 0.13 & 0.582 $\pm$ 0.15   & 0.688 $\pm$ 0.14\\
% \midrule
% SMGA    & 0.224 $\pm$ 0.11 & 0.569 $\pm$ 0.18   & 0.607 $\pm$ 0.17\\
% \bottomrule
% \end{tabular}
% \end{table}


\begin{table}
\centering
\caption{ SALAMI (Small)\label{results:salami:small}}
\small
\begin{tabular}{lrrr}
\toprule
Method & $F_{0.5}$ & $F_3$ & $F_\text{pair}$\\
\midrule
$L_2$   & 0.151 $\pm$ 0.11 & 0.195 $\pm$ 0.13   & 0.451 $\pm$ 0.19\\
$L_3$   & 0.171 $\pm$ 0.12 & 0.259 $\pm$ 0.16   & 0.459 $\pm$ 0.17\\
$L_4$   & 0.186 $\pm$ 0.12 & 0.315 $\pm$ 0.17   & 0.461 $\pm$ 0.15\\
$L_5$   & 0.195 $\pm$ 0.12 & 0.354 $\pm$ 0.17   & 0.455 $\pm$ 0.14\\
$L_6$   & 0.207 $\pm$ 0.12 & 0.391 $\pm$ 0.18   & 0.452 $\pm$ 0.13\\
$L_7$   & 0.214 $\pm$ 0.12 & 0.420 $\pm$ 0.18   & 0.445 $\pm$ 0.13\\
$L_8$   & 0.224 $\pm$ 0.12 & 0.448 $\pm$ 0.18   & 0.435 $\pm$ 0.13\\
$L_9$   & 0.229 $\pm$ 0.12 & 0.467 $\pm$ 0.18   & 0.425 $\pm$ 0.13\\
$L_{10}$& 0.234 $\pm$ 0.12 & 0.486 $\pm$ 0.18   & 0.414 $\pm$ 0.13\\
\midrule
$L$     & 0.192 $\pm$ 0.11 & 0.344 $\pm$ 0.15   & 0.448 $\pm$ 0.16\\
$L^*$   & 0.292 $\pm$ 0.15 & 0.525 $\pm$ 0.19   & 0.561 $\pm$ 0.16\\
\midrule
SMGA    & 0.173 $\pm$ 0.08 & 0.518 $\pm$ 0.12   & 0.493 $\pm$ 0.16\\
\bottomrule
\end{tabular}
\end{table}

\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figs/mconfusionfunc}
\caption{Confusion matrices illustrating the oracle selection of the number of segment
types $m\in [2, 10]$ for different pairs of metrics on SALAMI (functional). 
While $m=2$ is most frequently selected for all metrics, the large mass off-diagonal 
indicates that for a given track, a single fixed $m$ does not generally optimize all 
evaluation metrics.\label{mconfusion}}
\end{figure*}

\section{Conclusions}
The experimental results demonstrate that the proposed structural decomposition 
technique often generates solutions which achieve high scores on segmentation 
evaluation metrics.  However, automatically selecting a single ``best'' segmentation 
without a priori knowledge of the evaluation criteria remains a challenging practical 
issue.

\section{Acknowledgments}
The authors acknowledge support from The Andrew W. Mellon Foundation,
and NSF grant IIS-1117015.

\bibliography{refs}

\end{document}
